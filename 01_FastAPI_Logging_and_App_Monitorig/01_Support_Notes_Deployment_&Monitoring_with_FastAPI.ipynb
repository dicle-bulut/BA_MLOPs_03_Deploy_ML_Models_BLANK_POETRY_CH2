{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af5af19a",
   "metadata": {},
   "source": [
    "<img src=\"images/LandingPage-Header-RED-CENTRE.jpg\" alt=\"Notebook Banner\" style=\"width:100%; height:auto; display:block; margin-left:auto; margin-right:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adf9a89",
   "metadata": {},
   "source": [
    "# Introducing FastAPI\n",
    "\n",
    "Now we have a good grasp of writing high quality code and we have started our machine learning pipeline with MLflow. Let us now walk through a basic workflow, demonstrating how FastAPI can be used for deployment and monitoring ML models\n",
    "\n",
    "\n",
    "### What is FastAPI?\n",
    "\n",
    "What is FastAPI\n",
    "FastAPI is a modern, high-performance web framework for building APIs with Python designed for speed, both in development and execution, and comes with built-in features such as:\n",
    "\n",
    "Automatic data validation – FastAPI uses Pydantic models to automatically validate incoming request data based on defined types and constraints.\n",
    "Interactive API documentation – FastAPI generates real-time, interactive Swagger and ReDoc documentation from the code and data models.\n",
    "Dependency injection – FastAPI allows clean and reusable logic injection into endpoints using Python’s Depends, ideal for things like auth or database sessions.\n",
    "OAuth2 and JWT support – FastAPI has built-in tools for implementing secure OAuth2 authentication and JWT-based authorization workflows.\n",
    "Type-based routing and serialization – FastAPI leverages Python type hints to validate input, serialize output, and automatically generate API documentation.\n",
    "\n",
    "\n",
    "### Installation & Setup\n",
    "\n",
    "To work through this notebook you need to install all the dependacies from `requirements.txt` in your working enviromentd."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048fe805",
   "metadata": {},
   "source": [
    "## Understanding a Basic FastAPI App\n",
    "\n",
    "Let’s walk through a simple FastAPI example step by step. This will help you understand, how to inisitalise FastAPI, how to run FastAPI for the first time and will help you understand how fastapi can be used to run a basic web application.\n",
    "\n",
    "\n",
    " **1. First import the fast api library**\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "```\n",
    "**2. Initialise FastAPI using the command below**\n",
    "\n",
    "```python\n",
    "app = FastAPI()\n",
    "```\n",
    "\n",
    "\n",
    "**3. Define a test route**\n",
    "The code below is one of the many `requests` you can create on fastAPI this is an example of a simple `get` request \n",
    "```python\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"Hello, FastAPI is working!\"}\n",
    "```\n",
    "\n",
    "**What are the post and get requests?**\n",
    "A `GET` request is used to retrieve data from a server without changing any state or data.\n",
    "A `POST` request is used to send data to a server to create or update a resource.\n",
    "\n",
    "**4. Running FastAPI with Uvicorn**\n",
    "\n",
    "On the terminal you should have a file named `test_app.py`, you can start the server by navigating to the folder `03 Deploying & Productionising ML Models` and running the following command, `uvicorn [app_name]:app`--reload in a terminal:\n",
    "```bash\n",
    "uvicorn test_app:app --reload\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfefc055",
   "metadata": {},
   "source": [
    "### Find this code inside `fastapi_01_test_app.py` to see how to create a basic FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d88c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3845b79",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "You should see in the terminal:\n",
    "```bash\n",
    "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
    "INFO:     Started reloader process [11200] using StatReload\n",
    "INFO:     Started server process [24892]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "```\n",
    "\n",
    "copy and paste into the url bar on your browser `http://127.0.0.1:8000` (subject to change) or `hold ctrl` + `click` on the server it provides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829c6530",
   "metadata": {},
   "source": [
    "## What is Automatic Documentation\n",
    "\n",
    "FastAPI can automatically create API documentation following the OpenAPI standard. This includes:\n",
    "\n",
    "Swagger UI – An interactive, web-based interface for exploring and testing your API.\n",
    "Port: `http://127.0.0.1:8000/docs`\n",
    "\n",
    "ReDoc – A clean, alternative documentation interface.\n",
    "Port: `http://127.0.0.1:8000/redoc`\n",
    "\n",
    "The documentation is generated directly from your API’s code, ensuring it stays accurate and up to date with your endpoints, parameters, and data models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94d12b3",
   "metadata": {},
   "source": [
    "## How to Set Roles of Entry\n",
    "\n",
    "In many applications, different users have different levels of access. Examples of this are: **admins**, regular **users**, or **guests**. FastAPI allows you to manage this kind of access control using **dependency injection**.\n",
    "\n",
    "Instead of creating a database connection inside every route, you define a single “dependency” function that returns the connection, and FastAPI will inject it into your endpoints automatically when they run.\n",
    "```python\n",
    "def get_current_user_role():\n",
    "    # In a real app, you'd check a token or database\n",
    "    return \"user\"  # Try changing this to \"admin\"\n",
    "```\n",
    "\n",
    "\n",
    "### Example: Role-Based Access Control\n",
    "\n",
    "Let’s say we have a simple way to check the current user's role. We’ll use a dependency to simulate user authentication.\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI, Depends, HTTPException, status\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Simulated function to get a user's role (e.g., from a token or session)\n",
    "def get_current_user_role():\n",
    "    # In a real app, you'd check a token or database\n",
    "    return \"user\"  # Try changing this to \"admin\"\n",
    "\n",
    "# Dependency that checks if the user is an admin\n",
    "def require_admin(user_role: str = Depends(get_current_user_role)):\n",
    "    if user_role != \"admin\":\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_403_FORBIDDEN,\n",
    "            detail=\"Access denied: Admins only.\"\n",
    "        )\n",
    "\n",
    "# Open to everyone\n",
    "@app.get(\"/public\")\n",
    "def public_endpoint():\n",
    "    return {\"message\": \"This endpoint is open to all users.\"}\n",
    "\n",
    "# Protected route, only for admins\n",
    "@app.get(\"/admin\", dependencies=[Depends(require_admin)])\n",
    "def admin_endpoint():\n",
    "    return {\"message\": \"Welcome, Admin. You have access to this ro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a8f028",
   "metadata": {},
   "source": [
    "### Find this code inside fastapi_02_roles_app.py to see implementation of roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f1110",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c47e0",
   "metadata": {},
   "source": [
    "## How to Set Up a Health Configuration Endpoint\n",
    "\n",
    "Health check endpoints are useful for monitoring whether your application is alive, responsive, and ready to serve requests. They're especially important in production environments, where tools like load balancers, orchestration platforms (such as Kubernetes), or CI/CD pipelines may ping this endpoint to verify the application's health.\n",
    "\n",
    "In FastAPI, setting up a simple health check is straightforward using a `GET` route.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Health Configuration Endpoint\n",
    "\n",
    "The following example defines a `/health` endpoint that returns basic status information about your API. This includes:\n",
    "\n",
    "- `status`: Whether the app is running properly\n",
    "- `version`: The current API version\n",
    "- `service`: The name or identifier for the API\n",
    "- `dependencies`: A list or description of key services or libraries the app depends on\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Define the response schema using Pydantic\n",
    "class HealthResponse(BaseModel):\n",
    "    status: str\n",
    "    version: str\n",
    "    service: str\n",
    "    dependencies: str\n",
    "\n",
    "# GET endpoint for health check\n",
    "@app.get(\"/health\", response_model=HealthResponse)\n",
    "def health_check():\n",
    "    return {\n",
    "        \"status\": \"ok\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"service\": \"Titanic Predictor API\",\n",
    "        \"dependencies\": \"MLflow, Scikit-learn\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc005f6",
   "metadata": {},
   "source": [
    "### Find this code inside `fastapi_03_health_app.py` to see implementation of roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa0c15a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85a5e756",
   "metadata": {},
   "source": [
    "## Titanic Survival Prediction - Single Entry & Batch Entry\n",
    "\n",
    "This section defines the structure for an individual passenger, loads a machine learning model on FastAPI startup, and provides an endpoint to predict the survival of a single passenger.\n",
    "\n",
    "**1) Define the Passenger schema using Pydantic**\n",
    "\n",
    "Each incoming request for prediction must conform to this structure. FastAPI will validate and parse this automatically.\n",
    "\n",
    "```python\n",
    "class Passenger(BaseModel):\n",
    "    Pclass: int\n",
    "    Sex: str\n",
    "    Age: float\n",
    "    SibSp: int\n",
    "    Parch: int\n",
    "    Fare: float\n",
    "    Embarked: str\n",
    "\n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"Pclass\": 1,\n",
    "                \"Sex\": \"female\",\n",
    "                \"Age\": 24.0,\n",
    "                \"SibSp\": 0,\n",
    "                \"Parch\": 0,\n",
    "                \"Fare\": 75.0,\n",
    "                \"Embarked\": \"C\"\n",
    "            }\n",
    "        }\n",
    "```\n",
    "<br>\n",
    "\n",
    "\n",
    "**2) Load the ML model on FastAPI startup**\n",
    "\n",
    "The model is loaded using a specific MLflow run ID and artifact path, and stored in a global variable. If loading fails, it is best practice to logs the error and keeps the model as `None`.\n",
    "\n",
    "```python\n",
    "MLFLOW_RUN_ID = \"404582437544310156\"\n",
    "MODEL_ARTIFACT_PATH = \"models/m-91653bd6e0d14e0598c366aea5981528/artifacts\"\n",
    "model = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "def load_model():\n",
    "    global model\n",
    "    try:\n",
    "        current_dir = pathlib.Path(__file__).resolve().parent\n",
    "        model_path = current_dir.parent.parent / \"mlruns\" / MLFLOW_RUN_ID / MODEL_ARTIFACT_PATH\n",
    "        model_uri = model_path.as_uri()\n",
    "\n",
    "        print(f\"Loading model from: {model_uri}\")\n",
    "        model = mlflow.sklearn.load_model(model_uri)\n",
    "        print(\"Model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load model: {e}\")\n",
    "        model = None\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**3) Create single-entry endpoint**\n",
    "\n",
    "Next we need to make sure that FastAPI knows we are trying to create an endpoint (a URL path) that listens for POST requests at `/predict_single`. A POST request is typically used to send data to a server (like form input or JSON).\n",
    "\n",
    "`@app.post(\"/predict_single\")`\n",
    "\n",
    "<br>\n",
    "\n",
    "**4) Then define a function that will run when someone sends a POST request to `/predict_single`. It expects to receive a `Passenger` object, defining the structure of the input data (like name, age, sex, etc.).**\n",
    "\n",
    "`def predict_survival(passenger: Passenger):`\n",
    "\n",
    "<br>\n",
    "\n",
    "**5) Converts the incoming passenger data to a Pandas DataFrame, which the ML model expects.**\n",
    "\n",
    "`input_df = pd.DataFrame([passenger.dict()])`\n",
    "\n",
    "<br>\n",
    "\n",
    "**6) Makes the prediction: 1 = survived, 0 = did not survive.**\n",
    "\n",
    "`prediction = model.predict(input_df)[0]`\n",
    "\n",
    "<br>\n",
    "\n",
    "**7) Returns a JSON with both the number and a human-readable result.**\n",
    "\n",
    "```python\n",
    "return {\n",
    "    \"prediction\": int(prediction),\n",
    "    \"survival_status\": \"Survived\" if prediction == 1 else \"Not Survived\"\n",
    "}\n",
    "```\n",
    "\n",
    "If anything goes wrong, raise a proper HTTP error code:\n",
    "\n",
    "```python\n",
    "except Exception as e:\n",
    "    raise HTTPException(status_code=500, detail=f\"Prediction failed: {e}\")\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**8) Root endpoint to confirm API is running**\n",
    "\n",
    "You can quickly check if the API is live by visiting the root `/` endpoint in your browser or with a GET request.\n",
    "\n",
    "```python\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\"message\": \"Titanic MLflow API is running\"}\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**FastAPI will automatically:**\n",
    "- Validate the incoming JSON against the `Passenger` model.\n",
    "- Parse it into a structured Python object.\n",
    "- If validation fails, it returns a `422 Unprocessable Entity` error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f91d76a",
   "metadata": {},
   "source": [
    "### Find this code inside `fastapi_04_single_app.py` to see implementation of a single entry test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9ed089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5ca49f9",
   "metadata": {},
   "source": [
    "### How to implement Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b4387",
   "metadata": {},
   "source": [
    "**When creating input values for fastapi, it is best practice to add some validation to insure values are resonable**\n",
    "\n",
    "When you creating a class for our input data we can use Pydantic to run three steps:\n",
    "\n",
    "**Step 1 — Type Checking**\n",
    "\n",
    "`Literal[X, Y, Z]` - values must be one of the following form the list\n",
    "\n",
    "`float` - must be convertible to a float.\n",
    "\n",
    "`int` - must be convertible to an integer.\n",
    "\n",
    "**Step 2 — Constraint Checking**\n",
    "\n",
    "`Field()` is a helper function you use inside a BaseModel to give extra information about a model attribute beyond just the type hint\n",
    "\n",
    "`gt` - greater than.\n",
    "\n",
    "`lt` - less than.\n",
    "\n",
    "`ge` - greater than or equal to.\n",
    "\n",
    "`...` - required field.\n",
    "\n",
    "**Step 3 — Documentation Info**\n",
    "\n",
    "`description` helps us generate text to give more hints on how to enter the field(e.g., OpenAPI in FastAPI).\n",
    "\n",
    "```python\n",
    "class Passenger(BaseModel):\n",
    "    Pclass: Literal[1, 2, 3] = Field(..., description=\"Passenger class (1 = 1st, 2 = 2nd, 3 = 3rd)\")\n",
    "    Sex: Literal[\"male\", \"female\"] = Field(..., description=\"Sex of the passenger\")\n",
    "    Age: float = Field(..., gt=0, lt=100, description=\"Age must be between 0 and 100\")\n",
    "    SibSp: int = Field(..., ge=0, description=\"Number of siblings/spouses aboard\")\n",
    "    Parch: int = Field(..., ge=0, description=\"Number of parents/children aboard\")\n",
    "    Fare: float = Field(..., ge=0, description=\"Fare paid must be non-negative\")\n",
    "    Embarked: Literal[\"C\", \"Q\", \"S\"] = Field(..., description=\"Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\")\n",
    "\n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"Pclass\": 1,\n",
    "                \"Sex\": \"female\",\n",
    "                \"Age\": 24.0,\n",
    "                \"SibSp\": 0,\n",
    "                \"Parch\": 0,\n",
    "                \"Fare\": 75.0,\n",
    "                \"Embarked\": \"C\"\n",
    "            }\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e72c32b",
   "metadata": {},
   "source": [
    "### Find this code inside `fastapi_04_02_single_app.py` to see implementation of a single entry test (with validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d557945c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03e13619",
   "metadata": {},
   "source": [
    "## Using FastAPI to Make Multi Entry Tests Using MLflow\n",
    "\n",
    "**1) To create a multi-entry test define a data model like last time in out FastAPI script (test.py), but stucture the class in a way that recieves multiple entries**\n",
    "\n",
    "```python\n",
    "# --- Passenger schema ---\n",
    "class PassengerBatch(RootModel[List[Passenger]]):\n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": [\n",
    "                {\n",
    "                    \"Pclass\": 1,\n",
    "                    \"Sex\": \"female\",\n",
    "                    \"Age\": 24.0,\n",
    "                    \"SibSp\": 0,\n",
    "                    \"Parch\": 0,\n",
    "                    \"Fare\": 75.0,\n",
    "                    \"Embarked\": \"C\"\n",
    "                },\n",
    "                {\n",
    "                    \"Pclass\": 3,\n",
    "                    \"Sex\": \"male\",\n",
    "                    \"Age\": 22.0,\n",
    "                    \"SibSp\": 1,\n",
    "                    \"Parch\": 0,\n",
    "                    \"Fare\": 7.25,\n",
    "                    \"Embarked\": \"S\"\n",
    "                },\n",
    "                {\n",
    "                    \"Pclass\": 2,\n",
    "                    \"Sex\": \"female\",\n",
    "                    \"Age\": 30.0,\n",
    "                    \"SibSp\": 1,\n",
    "                    \"Parch\": 1,\n",
    "                    \"Fare\": 26.0,\n",
    "                    \"Embarked\": \"Q\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "```\n",
    "\n",
    "**2) Create multi-entry endpoint**\n",
    "\n",
    "Create a FastAPI endpoint that listens for POST requests at `/predict_batch`. This will be used to handle and predict multiple passengers in one request.\n",
    "\n",
    "`@app.post(\"/predict_batch\")`\n",
    "\n",
    "<br>\n",
    "\n",
    "**3) Define the function that will handle the batch prediction**\n",
    "\n",
    "This function is triggered when a POST request is sent to `/predict_batch`. It accepts a `PassengerBatch` object containing a list of passengers.\n",
    "\n",
    "`def predict_survival_batch(passengers: PassengerBatch):`\n",
    "\n",
    "<br>\n",
    "\n",
    "**4) Convert the incoming list of passengers into a Pandas DataFrame**\n",
    "\n",
    "The ML model expects input as a DataFrame, so the list of passengers is first converted.\n",
    "\n",
    "`input_df = pd.DataFrame([p.dict() for p in passengers.root])`\n",
    "\n",
    "<br>\n",
    "\n",
    "**5) Make predictions for each passenger in the batch**\n",
    "\n",
    "Run the ML model on the input DataFrame to generate predictions.\n",
    "\n",
    "`predictions = model.predict(input_df)`\n",
    "\n",
    "<br>\n",
    "\n",
    "**6) Format the results into a structured list of dictionaries**\n",
    "\n",
    "Each prediction is formatted with the passenger's index, the raw prediction (0 or 1), and a human-readable survival status.\n",
    "\n",
    "```python\n",
    "results = [\n",
    "    {\n",
    "        \"passenger_index\": i,\n",
    "        \"prediction\": int(pred),\n",
    "        \"survival_status\": \"Survived\" if pred == 1 else \"Not Survived\"\n",
    "    }\n",
    "    for i, pred in enumerate(predictions)\n",
    "]\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**7) Return the predictions as a JSON response**\n",
    "\n",
    "The response includes all predictions under the key `batch_predictions`.\n",
    "\n",
    "```python\n",
    "return {\"batch_predictions\": results}\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Error handling**\n",
    "\n",
    "If something goes wrong during prediction, a `500 Internal Server Error` is returned.\n",
    "\n",
    "```python\n",
    "except Exception as e:\n",
    "    raise HTTPException(status_code=500, detail=f\"Batch prediction failed: {e}\")\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Once again FastAPI will automatically:**\n",
    "- Validate the incoming JSON against the `PassengerBatch` schema.\n",
    "- Parse it into a structured Python object with typed fields.\n",
    "- Return a `422 Unprocessable Entity` error if the input data is invalid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179459f1",
   "metadata": {},
   "source": [
    "### Find this code inside `fastapi_05_batch_test_app.py`to see implementation of a batch entry test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afbc287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76f22ccd",
   "metadata": {},
   "source": [
    "## Calling FastAPI Endpoints as API Request (Single Entry)\n",
    "\n",
    "**With our FastAPI server is running at http://127.0.0.1:8000 you can safely call the FastAPI endpoints from within any Python script or notebook that's part of your MLflow workflow using the requests library.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39aba7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Prediction Status: 200\n",
      "Single Prediction Result: {'prediction': 1, 'survival_status': 'Survived'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# 1. Call /predict_single\n",
    "\n",
    "single_url = \"http://127.0.0.1:8000/predict_single\"\n",
    "\n",
    "single_passenger = {\n",
    "    \"Pclass\": 1,\n",
    "    \"Sex\": \"female\",\n",
    "    \"Age\": 24.0,\n",
    "    \"SibSp\": 0,\n",
    "    \"Parch\": 0,\n",
    "    \"Fare\": 75.0,\n",
    "    \"Embarked\": \"C\"\n",
    "}\n",
    "\n",
    "single_response = requests.post(single_url, json=single_passenger)\n",
    "\n",
    "print(\"Single Prediction Status:\", single_response.status_code)\n",
    "print(\"Single Prediction Result:\", single_response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f0996d",
   "metadata": {},
   "source": [
    "## Calling FastAPI Endpoints as APU Request (Multi Entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed31d93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch Prediction Status: 200\n",
      "Batch Prediction Result: {'batch_predictions': [{'passenger_index': 0, 'prediction': 1, 'survival_status': 'Survived'}, {'passenger_index': 1, 'prediction': 0, 'survival_status': 'Not Survived'}, {'passenger_index': 2, 'prediction': 1, 'survival_status': 'Survived'}]}\n"
     ]
    }
   ],
   "source": [
    "# 2. Call /predict_batch\n",
    "\n",
    "batch_url = \"http://127.0.0.1:8000/predict_batch\"\n",
    "\n",
    "batch_passengers = [\n",
    "    {\n",
    "        \"Pclass\": 1,\n",
    "        \"Sex\": \"female\",\n",
    "        \"Age\": 24.0,\n",
    "        \"SibSp\": 0,\n",
    "        \"Parch\": 0,\n",
    "        \"Fare\": 75.0,\n",
    "        \"Embarked\": \"C\"\n",
    "    },\n",
    "    {\n",
    "        \"Pclass\": 3,\n",
    "        \"Sex\": \"male\",\n",
    "        \"Age\": 22.0,\n",
    "        \"SibSp\": 1,\n",
    "        \"Parch\": 0,\n",
    "        \"Fare\": 7.25,\n",
    "        \"Embarked\": \"S\"\n",
    "    },\n",
    "    {\n",
    "        \"Pclass\": 2,\n",
    "        \"Sex\": \"female\",\n",
    "        \"Age\": 30.0,\n",
    "        \"SibSp\": 1,\n",
    "        \"Parch\": 1,\n",
    "        \"Fare\": 26.0,\n",
    "        \"Embarked\": \"Q\"\n",
    "    }\n",
    "]\n",
    "\n",
    "batch_response = requests.post(batch_url, json=batch_passengers)\n",
    "\n",
    "print(\"\\nBatch Prediction Status:\", batch_response.status_code)\n",
    "print(\"Batch Prediction Result:\", batch_response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a412a81",
   "metadata": {},
   "source": [
    "## Monitoring ML Projects\n",
    "Now we can make requests using FastAPI, but without proper monitoring, we have no visibility into how our application is performing. Monitoring is essential because it allows us to track the health, performance, and reliability of our API in real time. \n",
    "<br>\n",
    "\n",
    "**Why?**\n",
    "<br>\n",
    "It helps detect issues like high response times, failed predictions, or resource bottlenecks before they affect users or business operations. By collecting metrics with tools like Prometheus, we can make data-driven decisions, optimize system performance, and ensure our machine learning models are delivering accurate and timely results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731d4d73",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Before you start use the link to download the version of Prometheus suitable for your system https://prometheus.io/download/\n",
    "\n",
    "Windows Example: `prometheus-3.5.0.windows-amd64.zip` | `windows` | `amd64` | `119.22 MiB`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920319ad",
   "metadata": {},
   "source": [
    "#### Configure Prometheus\n",
    "After downloading and extracting Prometheus, copy/move the folder to the project path and open the file named `prometheus.yml`. This is the main configuration file where you define how Prometheus scrapes metrics.\n",
    "\n",
    "**Edit the configuration file**\n",
    "\n",
    "Replace the contents of `prometheus.yml` with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7be0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "global:\n",
    "  scrape_interval: 15s  # Set the scrape interval to every 15 seconds\n",
    "  evaluation_interval: 15s  # Evaluate rules every 15 seconds\n",
    "\n",
    "# Alertmanager configuration\n",
    "alerting:\n",
    "  alertmanagers:\n",
    "    - static_configs:\n",
    "        - targets:\n",
    "          # - alertmanager:9093\n",
    "\n",
    "# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.\n",
    "rule_files:\n",
    "  # - \"first_rules.yml\"\n",
    "  # - \"second_rules.yml\"\n",
    "\n",
    "# Scrape configuration\n",
    "scrape_configs:\n",
    "  - job_name: \"prometheus\"\n",
    "    static_configs:\n",
    "      - targets: [\"localhost:9090\"]\n",
    "        labels:\n",
    "          app: \"prometheus\"\n",
    "\n",
    "  - job_name: \"fastapi_app\"\n",
    "    static_configs:\n",
    "      - targets: [\"localhost:8000\"]\n",
    "        labels:\n",
    "          app: \"fastapi\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b444d33c",
   "metadata": {},
   "source": [
    "**What does this configuration do:**\n",
    "- Sets the global scrape interval and evaluation interval to 15 seconds.\n",
    "- Configures a job for Prometheus itself (localhost:9090), so it can monitor its own metrics.\n",
    "- Adds a job named fastapi_app that scrapes metrics from your FastAPI application on localhost:8000, where the /metrics endpoint is exposed.\n",
    "\n",
    "Save the file and return to the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671cbe12",
   "metadata": {},
   "source": [
    "#### **Start Prometheus**\n",
    "From the same directory as the config file, run Prometheus on the terminal, we suggest open a new terminal for this purpose.\n",
    "\n",
    "`./prometheus.exe`\n",
    "\n",
    "Keep it running in the background.\n",
    "To open Prometheus visit http://localhost:9090 in your browser.\n",
    "\n",
    "Note: On some OS's like windows some security preferences may cause windows to second guess the opening of the application. Click `more info` and then `run anyway`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cefcd9a",
   "metadata": {},
   "source": [
    "## What should we be monitoring with Prometheus?\n",
    "In this walkthrough we will be using prometheus to monitor two types of mertrics. Metrics related the perfomacnce of our app in relation to 'The Four Golden Signals' and Metrics related to requests and outputs of the model itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b10a7f2",
   "metadata": {},
   "source": [
    "#### The Four Golden Signals\n",
    "The four golden signals of monitoring are latency, traffic, errors, and saturation. If you can only measure four metrics of your user-facing system, focus on these four.\n",
    "\n",
    "`Latency` is the time it takes to process a request from start to finish. For example, you might see high latency on an API endpoint during peak hours when response times jump from 200ms to over 2 seconds.\n",
    "\n",
    "`Traffic` refers to the volume of incoming requests to your application over time. For instance, an e-commerce site might experience a spike in traffic during a flash sale, with thousands of requests per minute.\n",
    "\n",
    "`Errors` measure the rate or number of failed requests, such as HTTP 500 or 404 responses. You might see a surge in errors if a model file is missing or the database becomes unreachable.\n",
    "\n",
    "`Saturation` indicates how close a system is to its capacity, such as CPU, memory, or thread limits. A FastAPI app on a small server might show saturation when it handles more concurrent requests than it can efficiently support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c14f6",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<img src=\"images/four_golden.png\" alt=\"Datasource\" style=\"width:40%; height:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b323e653",
   "metadata": {},
   "source": [
    "## Implementing Prometheus for Monitoring\n",
    "\n",
    "**1) Import Library** \n",
    "\n",
    "To start using Prometheus with FastAPI, you first need to import the relevant metric types from the prometheus_client library\n",
    "\n",
    "- Counter: A metric that only increases—used for counting events like HTTP requests.\n",
    "- Gauge: A metric that can increase or decrease—used for values like CPU usage or memory consumption.\n",
    "- make_asgi_app: A function that creates an ASGI-compatible app which exposes all registered metrics.\n",
    "\n",
    "```python\n",
    "from prometheus_client import Counter, Gauge, make_asgi_app\n",
    "```\n",
    "\n",
    "**2) Mount Prometheus Metrics Endpoint**\n",
    "\n",
    "Once your metrics are defined, you need to expose them through an HTTP endpoint so Prometheus can collect them. FastAPI supports ASGI middleware, so we use make_asgi_app to expose a /metrics route.\n",
    "\n",
    "```python\n",
    "metric_app = make_asgi_app()\n",
    "app.mount(\"/metrics\", metric_app)\n",
    "```\n",
    "\n",
    "**3) Create Metric**\n",
    "\n",
    "In order to create a metric with prometheus there are 5 parts we have to look at:\n",
    "- Declare the variable `http_requests_total`\n",
    "- Call Prometheus Metric. Prometheus has four main metrics designed for a specific kind of measurement.\n",
    "    - Counter - You use it to count events\n",
    "    - Gauge - To track current state or resource usage\n",
    "    - Histogram - it counts how many observations fall into pre-defined buckets (ranges)\n",
    "    - Summary - Similar to a histogram, but it calculates quantiles\n",
    "\n",
    "```python\n",
    "http_requests_total = Counter()\n",
    "```\n",
    "\n",
    "**4) Inside the Prometheus Metric you need to provide:**\n",
    "- Metric Name: the unique name Prometheus uses to identify this metric.\n",
    "- Help Text: a human-readable description of what this metric tracks.\n",
    "- Labels: dimensions that allow you to break down the metric by HTTP method, route, and response status.\n",
    "\n",
    "```python\n",
    "http_requests_total = Counter(\n",
    "    \"http_requests_total\",\n",
    "    \"Total number of HTTP requests received, labeled by method, endpoint, and HTTP status code.\",\n",
    "    [\"method\", \"endpoint\", \"status_code\"]\n",
    ")\n",
    "\n",
    "http_request_duration_seconds = Counter(\n",
    "    \"http_request_duration_seconds_total\",\n",
    "    \"Total accumulated HTTP request duration in seconds, labeled by endpoint.\",\n",
    "    [\"endpoint\"]\n",
    ")\n",
    "\n",
    "http_errors_total = Counter(\n",
    "    \"http_errors_total\",\n",
    "    \"Total number of HTTP error responses (status code >= 500), labeled by method and endpoint.\",\n",
    "    [\"method\", \"endpoint\"]\n",
    ")\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "**5) Create middleware for collecting metrics**\n",
    "\n",
    "After seting up our metrics, for Prometheus to retrieve this data we need to set up some middleware for these server metrics. Middleware is a function or component that sits between the request coming in and the response going out.\n",
    "\n",
    "Think of it as a pipeline stage that runs before and after your route handlers.\n",
    "\n",
    "**6) Create a fastAPI wrapper for your middleware**\n",
    "\n",
    "We start by defining a middleware function using the @app.middleware(\"http\") decorator. This middleware will collect request-related data and update the Prometheus metrics accordingly.\n",
    "\n",
    "```python\n",
    "@app.middleware(\"http\")\n",
    "async def prometheus_metrics_middleware(request: Request, call_next):\n",
    "    start_time = time.time()\n",
    "    method = request.method\n",
    "    endpoint = request.url.path\n",
    "```\n",
    "- `@app.middleware(\"http\")`: Registers the function as HTTP middleware — runs on every request.\n",
    "- `request`: Request: Represents the incoming HTTP request.\n",
    "- `call_next`: A function that processes the request and returns a `Response`.\n",
    "- `start_time`: Captures when the request started, to calculate latency later.\n",
    "- `method`: The HTTP method (e.g., `GET`, `POST`).\n",
    "- `endpoint`: The path of the incoming request (e.g., `/predict_single`).\n",
    "\n",
    "\n",
    "**7) Processing the Request**\n",
    "This section forwards the incoming request to the appropriate route handler (e.g., /predict_single). It executes the logic defined in your endpoint and waits for the response.\n",
    "\n",
    "```python\n",
    "try:\n",
    "    response = await call_next(request)\n",
    "```\n",
    "- call_next(request) processes the request through the FastAPI application and returns a response.\n",
    "- await is used because this is an asynchronous call.\n",
    "- The response object contains the status code, which we'll use for metric labeling.\n",
    "\n",
    "\n",
    "**8) Handling Exceptions and Errors**\n",
    "\n",
    "If something goes wrong during request processing (e.g., an unhandled exception occurs in a route), the application should catch it and record it as an error.\n",
    "\n",
    "```python\n",
    "    except Exception:\n",
    "        http_errors_total.labels(method=method, endpoint=endpoint).inc()\n",
    "        raise\n",
    "```\n",
    "- This block catches exceptions, increments the http_errors_total counter to log a failure, and re-raises the exception so FastAPI can handle it properly.\n",
    "- The metric is labeled with the HTTP method and endpoint to provide context about where the error happened.\n",
    "\n",
    "**9) Updating Prometheus Metrics**\n",
    "\n",
    "Once the request has been processed successfully (or failed), we update our metrics.\n",
    "\n",
    "```python\n",
    "    http_requests_total.labels(\n",
    "        method=method,\n",
    "        endpoint=endpoint,\n",
    "        status_code=str(status_code)\n",
    "    ).inc()\n",
    "\n",
    "    http_request_duration_seconds.labels(endpoint=endpoint).observe(duration)\n",
    "\n",
    "    if status_code >= 500:\n",
    "        http_errors_total.labels(method=method, endpoint=endpoint).inc()\n",
    "```\n",
    "- http_requests_total: Increments the counter for each completed request, labeled by method, path, and response status.\n",
    "\n",
    "- http_request_duration_seconds: Records how long the request took to complete. This metric uses a histogram to bucket duration values.\n",
    "\n",
    "- If the response status code is 500 or greater, we increment the http_errors_total counter again (useful if the exception didn’t trigger but an error still occurred).\n",
    "\n",
    "*note: we use the status codes for 500 or greater as this is the status code for server-side errors*\n",
    "\n",
    "**10) Return Response**\n",
    "\n",
    "This line registers the function prometheus_metrics_middleware as HTTP middleware in your FastAPI application.\n",
    "\n",
    "\n",
    "Middleware acts as a wrapper around your request-response cycle. and allows us to:\n",
    "- Record when the request starts\n",
    "- Execute the request\n",
    "- Measure how long it took\n",
    "- Log status codes and errors\n",
    "- Update Prometheus metrics for observability and monitoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d133d3bb",
   "metadata": {},
   "source": [
    "### Find this code inside: `prometheus_00_simple_monitoring_app` to see how prometheus can track a simple metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daa24d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fae59acc",
   "metadata": {},
   "source": [
    "### Find this code inside: `prometheus_01_four_signal_monitor_app` to see how we can track the four golden signals using Prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06166f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8c998d7",
   "metadata": {},
   "source": [
    "## **More Custom Metrics**\n",
    "\n",
    "These custom metrics allow you to track detailed information specific to your machine learning model. For example, you can monitor how many predictions are being made, the output classes being predicted, how often batch vs single predictions are used, and the age distribution of passengers being processed.\n",
    "\n",
    "We do not need to create middleware for these metrics because they are directly tied to specific actions (like running a prediction), not to every HTTP request. Instead, these metrics should be updated inside your route functions where the relevant logic occurs.\n",
    "\n",
    "\n",
    "```python   \n",
    "titanic_predictions_total = Counter(\n",
    "    \"titanic_predictions_total\", \n",
    "    \"Total number of predictions made by the model\"\n",
    ")\n",
    "titanic_predictions_output = Counter(\n",
    "    \"titanic_predictions_output\", \n",
    "    \"Number of predictions made by predicted class\", [\"predicted_class\"]\n",
    ")\n",
    "prediction_type_total = Counter(\n",
    "    \"prediction_type_total\", \n",
    "    \"Counts of prediction requests by type\", [\"type\"]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aafe6d0",
   "metadata": {},
   "source": [
    "### Find this code inside: `prometheus_02_model_monitor_app` to see how we cam use prometheus to monitor metrics specific to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbd0011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac092534",
   "metadata": {},
   "source": [
    "## How to Query Prometheus\n",
    "\n",
    "### Using Prometheus\n",
    "\n",
    "Now that we have integrated metrics into our FastAPI application, the next step is to understand how to view and use these metrics for monitoring and analysis.\n",
    "\n",
    "To use Prometheus effectively, follow these steps:\n",
    "\n",
    "**1) Navigate to Prometheus**\n",
    "\n",
    "Start your application and confirm that the /metrics endpoint is accessible. You can test this by visiting http://localhost:8000/metrics in your browser. You should see a plain text list of metrics, each with names, labels, and current values.\n",
    "\n",
    "<img src=\"images/prometheus_metrics.png\" alt=\"Datasource\" style=\"width:60%; height:auto;\">\n",
    "\n",
    "```bash\n",
    "# HELP <metric_name> <description of what this metric tracks>\n",
    "# TYPE <metric_name> counter\n",
    "<metric_name>{<label_key1>=\"<label_value1>\", <label_key2>=\"<label_value2>\"} <value>\n",
    "<metric_name>{<label_key1>=\"<another_value1>\", <label_key2>=\"<another_value2>\"} <value>\n",
    "\n",
    "# HELP model_prediction_output Number of predictions made by predicted class\n",
    "# TYPE model_prediction_output counter\n",
    "titanic_prediction_output{predicted_class=\"1\"} 3.0\n",
    "titanic_prediction_output{predicted_class=\"0\"} 5.0\n",
    "```\n",
    "\n",
    "**2) Ensure Prometheus is open with the correct configuration**\n",
    "\n",
    "Prometheus must be running and properly configured to scrape your FastAPI app. This means you should have a prometheus.yml file that includes your application's address (localhost:8000) under scrape_configs.\n",
    "\n",
    "<img src=\"images/prometheus_application_nav.png\" alt=\"Datasource\" style=\"width:60%; height:auto;\">\n",
    "<img src=\"images/prometheus_application.png\" alt=\"Datasource\" style=\"width:60%; height:auto;\">\n",
    "\n",
    "**3) Access the Prometheus dashboard**\n",
    "Once Prometheus is running, navigate to http://localhost:9090. This is the Prometheus web UI where you can query and visualize metrics.\n",
    "\n",
    "**4) Write PromQL queries to inspect metrics**\n",
    "\n",
    "In the Prometheus UI, use the query bar to write PromQL expressions like:\n",
    "\n",
    "- `http_requests_total` to see all HTTP requests received\n",
    "- `model_predictions_total` to track total predictions\n",
    "- `prediction_type_total` to compare single vs batch prediction frequency\n",
    "- `passenger_age_distribution` to view the histogram of input ages\n",
    "\n",
    "**5) Click on `Charts` to metrics over time**\n",
    "\n",
    "Prometheus stores historical values, so you can view trends, spikes, or anomalies over different time windows. Use the graph tab to visualize how a metric changes over time.\n",
    "\n",
    "\n",
    "<img src=\"images/promquery.png\" alt=\"Datasource\" style=\"width:100&%; height:auto;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb3034f",
   "metadata": {},
   "source": [
    "## Logging predictions with Prometheus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83145905",
   "metadata": {},
   "source": [
    "Now we have a FastAPI application that includes Prometheus metrics for monitoring model predictions, prediction types, and passenger age distribution.\n",
    "Another feature we can add is to track usage stats with Prometheus this is helpful for visual dashboards and performance monitoring (e.g., in Grafana).\n",
    "\n",
    "Prometheus automatically scrapes metrics exposed by the FastAPI app. Every time a prediction is made, we update counters and histograms that represent model behavior.\n",
    "\n",
    "**Log to Prometheus in the prediction endpoint**\n",
    "\n",
    "```python\n",
    "    titanic_predictions_total.inc()\n",
    "    titanic_predictions_output.labels(predicted_class=str(prediction)).inc()\n",
    "    prediction_type_total.labels(type=\"single\").inc()\n",
    "    passenger_age_distribution.observe(passenger.Age)\n",
    "```\n",
    "\n",
    "`titanic_predictions_total.inc()`\n",
    "- Increments a counter every time a prediction is made.\n",
    "- Example: titanic_predictions_total = 21\n",
    "\n",
    "`titanic_predictions_output.labels(predicted_class=str(prediction)).inc()`\n",
    "- Increments a labelled counter that tracks how many predictions were class 0 or 1.\n",
    "- Example:\n",
    "predicted_class=\"0\" : 7\n",
    "predicted_class=\"1\" : 14\n",
    "\n",
    "`prediction_type_total.labels(type=\"single\").inc()`\n",
    "- Increments a labelled counter for the type of prediction (e.g., \"single\" vs \"batch\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eca568c",
   "metadata": {},
   "source": [
    "### Logging as a JSON\n",
    "\n",
    "Once prometheus is tracking our metrics it will also be convenient for us to save/log the data as a JSON file (JSONL)\n",
    "\n",
    "1) Add a helper to increment an execution counter\n",
    "\n",
    "```python\n",
    "    def get_next_execution_number(log_path: pathlib.Path) -> int:\n",
    "    if not log_path.exists():\n",
    "        return 1\n",
    "    try:\n",
    "        with open(log_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            if not lines:\n",
    "                return 1\n",
    "            last_line = lines[-1]\n",
    "            last_log = json.loads(last_line)\n",
    "            return last_log.get(\"execute\", 0) + 1\n",
    "    except Exception:\n",
    "        return 1\n",
    "```\n",
    "\n",
    "What it does:\n",
    "- If the log file is missing or empty, `returns 1.`\n",
    "- Otherwise, parses the last JSON line and returns `last.execute` + `1`\n",
    "\n",
    "<br>\n",
    "\n",
    "2) Decide where the JSONL file will live (set on startup)\n",
    "\n",
    "In your `@app.on_event(\"startup\")` you can compute the model path and then set a timestamped (using: `timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")`), then log path next to the model `artifacts` file. This ensures logs for each process run are kept separate.\n",
    "\n",
    "\n",
    "```python\n",
    "current_dir = pathlib.Path(__file__).resolve().parent\n",
    "model_path = current_dir.parent.parent / \"mlruns\" / EXPERIMENT_ID / MODEL_ARTIFACT_PATH\n",
    "\n",
    "# Optional custom override (uncomment & set CUSTOM_LOG_PATH globally if desired)\n",
    "# CUSTOM_LOG_PATH = pathlib.Path(\"/your/custom/path/inference_logs.jsonl\")\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "PREDICTION_LOG_PATH = (\n",
    "    CUSTOM_LOG_PATH if \"CUSTOM_LOG_PATH\" in globals() and CUSTOM_LOG_PATH is not None\n",
    "    else model_path / f\"simulation_logs_{timestamp}.jsonl\"\n",
    ")\n",
    "\n",
    "PREDICTION_LOG_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(PREDICTION_LOG_PATH, \"w\"):\n",
    "    pass  reset file each startup\n",
    "\n",
    "```\n",
    "\n",
    "3) Build the JSON object laying out how it will be structured.\n",
    "\n",
    "```python\n",
    "execution_number = get_next_execution_number(PREDICTION_LOG_PATH)\n",
    "log_entry = {\n",
    "    \"execute\": execution_number,\n",
    "    \"execution_time\": datetime.now().isoformat(),  # ISO timestamp\n",
    "    \"experiment_id\": EXPERIMENT_ID,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"prediction\": int(prediction),\n",
    "    \"probability\": {\n",
    "        \"Not Survived\": float(probabilities[0]),\n",
    "        \"Survived\": float(probabilities[1]),\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "4) Append one JSON object per line (JSONL)\n",
    "```python\n",
    "with open(PREDICTION_LOG_PATH, \"a\") as f:\n",
    "    json.dump(log_entry, f)\n",
    "    f.write(\"\\n\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8b6b18",
   "metadata": {},
   "source": [
    "### Find this code inside `prometheus_03_logging_app.py` to see the implementation of logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052bb9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9513cd87",
   "metadata": {},
   "source": [
    "## Multi Execute Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa34372",
   "metadata": {},
   "source": [
    "To demonstrate logging operating larger scale below a version of the code we have been working with that will perform multiple executes of random values to provide us more logs.\n",
    "\n",
    "\n",
    "We do this by creating a new endpoint called `.post(\"/simulate_predictions\"` containing\n",
    "\n",
    "```python\n",
    "        for _ in range(10):\n",
    "            passenger = {\n",
    "                \"Pclass\": random.choice([1, 2, 3]),\n",
    "                \"Sex\": random.choice([\"male\", \"female\"]),\n",
    "                \"Age\": round(random.uniform(1, 80), 1),\n",
    "                \"SibSp\": random.randint(0, 3),\n",
    "                \"Parch\": random.randint(0, 3),\n",
    "                \"Fare\": round(random.uniform(10, 250), 2),\n",
    "                \"Embarked\": random.choice([\"C\", \"Q\", \"S\"])\n",
    "            }\n",
    "```\n",
    "\n",
    "This will randomly select values for the model to parse 10 times "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f572ec15",
   "metadata": {},
   "source": [
    "#### Find this code in `prometheus_04_multi_execute_app.py` demonstating simulating multiple executes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ffef9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f31e570e",
   "metadata": {},
   "source": [
    "## FastAPI Query: Input Simulations\n",
    "\n",
    "For further control of the simulations being ran it might be in your best interest to have a way to input how many inputs the simulation can take.\n",
    "\n",
    "`from fastapi import Query`\n",
    "\n",
    "`Query` in FastAPI lets you declare a parameter that will be read from the request’s URL query string. FastAPI automatically extracts that value, validates it against any rules you set, and converts it to the correct Python type. This means you can use the parameter directly in your function without manually parsing or checking the request.\n",
    "\n",
    "\n",
    "`num_executions: int = Query(10, ge=1, le=100, description=\"How many simulations to run\")`\n",
    "\n",
    "Query can take multiple arguments like: default values (10) and validation (ge (greater than equal to) & le (less than equal to))\n",
    "\n",
    "**Note: Validation is important these parameter can be used as injections directly into our code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a2e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0711533c",
   "metadata": {},
   "source": [
    "# Grafana - Monitoring Visualisation Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eafa90",
   "metadata": {},
   "source": [
    "Now we have Prometheus running tracking metrics, it will be useful to create visualisations to see these metrics clearer. For this we will be using Grafana.\n",
    "\n",
    "#### What is Grafana\n",
    "\n",
    "Grafana is an open-source analytics and visualization platform used to monitor and display time-series data from various data sources like Prometheus, InfluxDB, and Elasticsearch. It allows users to create interactive and customizable dashboards for real-time observability and performance monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251118bf",
   "metadata": {},
   "source": [
    "#### How to run Grafana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06fa99e",
   "metadata": {},
   "source": [
    "1. Download Stand alone Grafana Binaries. Unzip the file and place it in the project folder. \n",
    "\n",
    "Download from [grafana.com/grafana/download](https://grafana.com/grafana/download) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33094f59",
   "metadata": {},
   "source": [
    "2. Open Grafana\n",
    "\n",
    "Open a new terminal and navigate to grafana bins folder. on the new terminal type:\n",
    "\n",
    "`.grafana-server.exe`\n",
    "\n",
    "Using the app url with the port `3000` e.g: `http://127.0.0.1`:<span style=\"color:green;\">`3000`</span>\n",
    "\n",
    "*Note: Default Username: admin  Default Password: admin (you’ll be prompted to change the password, skip).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d64707c",
   "metadata": {},
   "source": [
    "3. Add a Data Source\n",
    "\n",
    "- Under `Connections` click `Data Sources`\n",
    "- Click on the `Add new data sources` from the top right\n",
    "- Select `Prometheus`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcda8c5",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<img src=\"images/data_source.png\" alt=\"Datasource\" style=\"width:20%; height:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3bc675",
   "metadata": {},
   "source": [
    "4. Setting for Grafana\n",
    "\n",
    "Under Connections in the `Add new data sources` setting change `Prometheus server URL:` to the app URL (`http://127.0.0.1/`) + `:9090`\n",
    "\n",
    "<img src=\"images/prometheus_settings.png\" alt=\"Datasource\" style=\"width:40%; height:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8666aa89",
   "metadata": {},
   "source": [
    "save & exit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2ff4a3",
   "metadata": {},
   "source": [
    "5. Import Dashboard Template\n",
    "\n",
    "Dashboard templates can be made from scratch but we will be using a premade simple dashboard preconstructed using a JSON file\n",
    "\n",
    "- On the left side navigation click on `Dashboards` open the dropdown `New` and select `Import`\n",
    "- Within this training material folder open another folder folder called `grafana_dashboard`, select `Titanic API - Simple App Monitoring` and `load`\n",
    "\n",
    "<img src=\"images/new_dashboard.png\" alt=\"Datasource\" style=\"width:40%; height:auto;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508902eb",
   "metadata": {},
   "source": [
    "6. Open Grafana Dashboard\n",
    "\n",
    "<img src=\"images/grafana_dash.png\" alt=\"Datasource\" style=\"width:80%; height:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f859939c",
   "metadata": {},
   "source": [
    "#### What is Grafana doing? #### \n",
    "\n",
    "Grafana uses a language called PromQL (Prometheus Query Language), to query and visualize time-series metrics stored in a Prometheus database.\n",
    "\n",
    "When connected to Prometheus as a data source, Grafana allows users to write PromQL queries to extract specific metrics (like API uptime, request counts, error rates, etc.) and then visualizes them through interactive dashboards using panels like time series charts, gauges, heatmaps, and bar graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55da9648",
   "metadata": {},
   "source": [
    "To See this:\n",
    "\n",
    "On the top right on any visualisation click on the menu button to view inside the visualisation `⋮`\n",
    "\n",
    "<img src=\"images/edit_vis.png\" alt=\"Datasource\" style=\"width:15%; height:auto;\">\n",
    "\n",
    "Here we can see the PromQL in action at the bottom with `titanic_predictions_total` being the metric that tracks how many prediction our titanic prediction app has made:\n",
    "\n",
    "<img src=\"images/promql.png\" alt=\"Datasource\" style=\"width:50%; height:auto;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
